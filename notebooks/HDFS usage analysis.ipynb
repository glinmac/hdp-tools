{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDFS Usage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import json\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, date, time\n",
    "from os.path import basename, dirname, join as pjoin\n",
    "from pandas.io.json import json_normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import re \n",
    "from collections import defaultdict\n",
    "import humanize\n",
    "import xml.etree.cElementTree as ET\n",
    "%matplotlib inline\n",
    "#print(plt.style.available)\n",
    "plt.style.use('seaborn-dark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# default block size in bytes for HDFS\n",
    "BLOCK_SIZE = 128*1024.*1024.1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Analysis\n",
    "\n",
    "Quick overview on some of the properties of files in HDFS\n",
    "\n",
    "### Data acquisition\n",
    "\n",
    "This is based on taking a simple state of the HDFS filesystem listing all the files:\n",
    "\n",
    "```\n",
    "hdfs dfs -ls -R > hdfs.txt\n",
    "```\n",
    "\n",
    "The format of the file is\n",
    "```\n",
    "drwxr-xr-x   - user group            0 2015-09-04 15:56 /some/path/in/hdfs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_file(filename):\n",
    "    \"\"\"Build a list of object for each file in the HDFS filesystem\"\"\"\n",
    "    \n",
    "    record_re = re.compile(r'(?P<permissions>\\S+)\\s+(?:\\S+)\\s+(?P<user>\\S+)\\s+(?P<group>\\S+)\\s+(?P<size>\\d+)\\s+(?P<date>[\\d+-:\\s]+)\\s+(?P<path>\\S+)')\n",
    "\n",
    "    metadata = []\n",
    "\n",
    "    # to keep track of the total size of directories\n",
    "    directory_usage = defaultdict(int)\n",
    "    directories = []\n",
    "            \n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            m = record_re.match(line)\n",
    "            o = m.groupdict()\n",
    "            data = {\n",
    "                    'user': o['user'],\n",
    "                    'group': o['group'],\n",
    "                    'size': int(o['size']),\n",
    "                    'path': o['path'],\n",
    "                    'date': datetime.strptime(o['date'], '%Y-%m-%d %H:%M'),\n",
    "                    'permissions': o['permissions'],\n",
    "                    'is_dir': o['permissions'][0] == 'd',\n",
    "                    'is_userdir': o['path'].startswith('/user') and len(o['path'].split('/')) == 3,\n",
    "                    'dir_size': 0\n",
    "            }\n",
    "            \n",
    "            if o['permissions'][0] == 'd':\n",
    "                # stash the directory for later once we'll have visited all the children\n",
    "                directories.append(data)\n",
    "            else:\n",
    "                # add the size of the file to all upper directories\n",
    "                path = '/'                \n",
    "                for el in dirname(data['path']).split('/'):\n",
    "                    path = pjoin(path, el)\n",
    "                    directory_usage[path] += data['size']\n",
    "\n",
    "                metadata.append(data)\n",
    "\n",
    "    # add the directories and update its size\n",
    "    for d in directories:\n",
    "        d['dir_size'] = directory_usage[d['path']]\n",
    "        metadata.append(d)\n",
    "        \n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_data = parse_file('data/ch.hdfs')\n",
    "data = json_normalize(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Statistics')\n",
    "print('- %d files' % len(data))\n",
    "print('- %s used' % humanize.naturalsize(data['size'].sum()))\n",
    "print('- %d users' % len(data['user'].unique()))\n",
    "print('- average file size:', humanize.naturalsize(data['size'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage by user (ownership of files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax1 = data.groupby('user')['size']\\\n",
    "    .sum()\\\n",
    "    .apply(lambda x:round(x/1024./1024/1024))\\\n",
    "    .sort_values(ascending=False)\\\n",
    "    .head(10)\\\n",
    "    .plot(kind='barh', title='Top HDFS usage by user ownership')\n",
    "ax1.set_xlabel('Size in GB')\n",
    "ax1.set_ylabel('')\n",
    "ax1.invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage by path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax2 = data[data['is_dir']][['path', 'dir_size']]\\\n",
    "    .set_index('path')['dir_size']\\\n",
    "    .apply(lambda x: round(x/1024./1024./1024.))\\\n",
    "    .sort_values(ascending=False)\\\n",
    "    .head(10)\\\n",
    "    .plot(kind='barh', title='Top HDFS usage by path')\n",
    "ax2.set_xlabel('Size in GB')\n",
    "ax2.set_ylabel('')\n",
    "ax2.invert_yaxis()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top usage for /user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax = data[data['is_userdir']][['path', 'dir_size']]\\\n",
    "    .set_index('path')['dir_size']\\\n",
    "    .apply(lambda x: round(x/1024./1024./1024.))\\\n",
    "    .sort_values(ascending=False)\\\n",
    "    .head(10)\\\n",
    "    .plot(kind='barh', title='Top HDFS usage by path for /user')\n",
    "ax.invert_yaxis()\n",
    "ax.set_ylabel('')\n",
    "ax.set_xlabel('Size in GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top user with small files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = data[(data['is_dir'] == False) & (data['size'] < BLOCK_SIZE)]\\\n",
    "    .groupby('user')\\\n",
    "    .size()\\\n",
    "    .sort_values(ascending=False)\\\n",
    "    .head(10)\\\n",
    "    .plot(kind='barh')\n",
    "ax.set_xlabel('Number of files < HDFS block size')\n",
    "ax.set_ylabel('')\n",
    "ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average file size per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = data[(data['is_dir'] == False)]\\\n",
    "    .groupby('user')['size']\\\n",
    "    .mean()\\\n",
    "    .sort_values(ascending=False)\\\n",
    "    .apply(lambda x: x/1024./1024.)\\\n",
    "    .plot(kind='barh')\n",
    "ax.set_xlabel('Average file size per user (MB)')\n",
    "ax.set_ylabel('')\n",
    "ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top directories with biggest number of small files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp = data[(data['is_dir'] == False) & (data['size'] < BLOCK_SIZE)]['path'].apply(lambda x: dirname(x))\n",
    "ax = temp.groupby(temp).size()\\\n",
    "    .sort_values(ascending=False)\\\n",
    "    .head(10)\\\n",
    "    .plot(kind='barh')\n",
    "ax.set_ylabel('')    \n",
    "ax.set_xlabel('Number of small file (< HDFS Block Size)')\n",
    "ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed analysis\n",
    "\n",
    "### Data acquisition\n",
    "\n",
    "This relies on the raw fsimage from HDFS:\n",
    "\n",
    "```\n",
    "hdfs dfsadmin -fetchimage fsImage\n",
    "hdfs oiv -i fsImage -o fsImage.xml -p XML\n",
    "```\n",
    "\n",
    "This will produce an XML formatted version of the HDFS metadata\n",
    "\n",
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = ET.parse('data/hdfs.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "root = a.getroot()\n",
    "root.getchildren()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_xml(filename):\n",
    "    metadata = []\n",
    "    cnt = 0\n",
    "    for a in root.findall('INodeSection/inode'):\n",
    "        o = {\n",
    "                'inode_id': int(a.find('id').text),\n",
    "                'type': a.find('type').text,\n",
    "                'name': a.find('name').text\n",
    "        }\n",
    "        if o['type'] == 'FILE':\n",
    "            o['replication'] = int(a.find('replication').text)\n",
    "            o['perferredBlockSize'] = int(a.find('perferredBlockSize').text)       \n",
    "        file_size, num_block = 0, 0\n",
    "        if a.find('blocks'):\n",
    "            for block in a.find('blocks').getchildren():\n",
    "                file_size += int(block.find('numBytes').text)\n",
    "                num_block += 1\n",
    "            o['file_size'] = file_size\n",
    "            o['num_block'] = num_block\n",
    "        metadata.append(o)\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = json_normalize(parse_xml('data/hdfs.xml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference between 2 clusters\n",
    "\n",
    "Naively rely on 2 extracts for each cluster using \n",
    "\n",
    "```\n",
    "(cluster1) hdfs dfs -ls -R / > hdfs-1.txt\n",
    "(cluster2) hdfs dfs -ls -R / > hdfs-2.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_style": "center",
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def analyze_diff(path=None):\n",
    "    \"\"\"\n",
    "    tool to analyze differences between the 2 clusters\n",
    "    \n",
    "    If path is not None, the analysis is limited to the path given\n",
    "    \"\"\"\n",
    "    missing_files = []\n",
    "    different_files = []\n",
    "    properties = ['user', 'size', 'group', 'permissions']\n",
    "\n",
    "    for f in cluster_1:\n",
    "        # we skip the file if it is not in the path we are interested into\n",
    "        if path and not f['path'].startswith(path):\n",
    "            continue\n",
    "        if f['path'] not in index_2:\n",
    "            missing_files.append(f['path'])\n",
    "        else:\n",
    "            # compare\n",
    "            f2 = index_2[f['path']]\n",
    "            diff = []        \n",
    "            for p in properties: \n",
    "                try:\n",
    "                    if f[p] != f2[p]:                \n",
    "                        diff.append(p)\n",
    "                except:\n",
    "                    print(f)\n",
    "                    print(f2)\n",
    "            if diff:\n",
    "                different_files.append({\n",
    "                        'source': f,\n",
    "                        'target': f2,\n",
    "                        'diff': diff\n",
    "                    })\n",
    "    return (missing_files, different_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data for each cluster and build a quick index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_1 = parse_file('data/hdfs-1.txt')\n",
    "cluster_2 = parse_file('data/hdfs-2.txt')\n",
    "\n",
    "# build an index by path\n",
    "index_1 = {}\n",
    "for f in cluster_1:\n",
    "    index_1[f['path']] = f\n",
    "index_2 = {}\n",
    "for f in cluster_2:\n",
    "    index_2[f['path']] = f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "missing_files, different_files = analyze_diff('/user/glinmac')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display some output for missing files in cluster 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Missing files', len(missing_files))\n",
    "missing_dirs = []\n",
    "for f in missing_files:\n",
    "    d = dirname(f)\n",
    "    if d not in missing_dirs:\n",
    "        missing_dirs.append(d)\n",
    "        print('\\t%s' % d)\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display files that have different metadata properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('\\nFiles with different properties', len(different_files))\n",
    "for f in different_files:\n",
    "    print('\\t%s' % f['source']['path'])\n",
    "    for p in f['diff']:\n",
    "        print('\\t\\t%s: source=%s target=%s' %(p, f['source'][p], f['target'][p]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "toc_position": {
   "height": "570px",
   "left": "0px",
   "right": "1068px",
   "top": "106px",
   "width": "212px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
